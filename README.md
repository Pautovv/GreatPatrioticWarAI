## 1. Введение

В данном репозетории представленна machine-learning часть продукта, разработанного для выступления на **Хакатоне победы** (от РТУ МИРЭА)
**Целью данного хакатона** являлась разработка веб-приложения с тематикой Великой Отечественной Войны. **Актуальность данной темы** - сохранение памяти о павших солдатах в современной и интерактивной форме.

## 2. Стэк техноголий

* `hugging face transformers` - для использования предобученных моделей для работы с текстом
* `hugging face diffusers` - для использования предобученных моделей для работы с изображениями
* `torch` - фундаментальная библиотека глубокого обучения
* `detoxify` - предобученная нейросеть для определения токстичности в тексте
* `python_tools` - инструмент для постобработки текста

## 3. Про модели

### **text generation**
Изначально была взята предобученная модель `ai-forever/rugpt3medium_based_on_gpt2`. Но сгенерированный ей текст был очень плохого качества. Поэтому было принято решение дообучить (В папке `fine_tuning` лежит код для обучения `ai-forever/rugpt3medium_based_on_gpt2`) данную модель на датасете `ptvnck/GPW_stories` (его я собрал из сборника "100 военных рассказов"). Так же была добавленна качественная постобаботка сгенерированного текста, а именно разбиение на абзацы, орфография, а так же замена слэнговых слов нашего времени на соответсвующие по смыслу слова военного времени. `detoxify` проверял текст на токсичность и в случае большого колличества токсичных слов они заменялись литературными синонимами. 

### **picture generartion** (данный пайплан я составлял вместе с моим коллегой [Miroslav Makarov](https://github.com/xevergreenx))
За основу была взято диффузионная модель `stable-diffusion`. В данном пайплайне для улучшения качества генерации картинок используется `Helsinki-NLP/opus-mt-ru-en` которая переводит промпт пользователя на английский язык (так как данной моделе комфортнее работать на английском языке). Так же идет поиск по заранее заготовленным сниппетам тем и существует заранее заготовленный промпт который задает определенные правила генерации (стилистику изображения). `j-hartmann/emotion-english-distilroberta-base` используется для анализа эмоциального профиля дневника. 

### **music generation**
В качестве модели была взята `facebook/musicgen-small`. Так же как и при генерации картинки используется `Helsinki-NLP/opus-mt-ru-en` для перевода промпта на английский язык. Изначально планировалось генерировать композицию длинной в 1 минуту, но модель генерировала это слишком долго поэтому мы остановились на 10 секундах. 


## 4. Будущее развитие данного проекта

Вообще у нас так же были подготовленны серврная часть и клиентская часть веб-приложения куда должны были быть подключенны данные модели, но к сожалению по техническим причинам мы не смогли этого сделать до защиты хакатона. Но в будущем я планирую самостоятельно добавить фулстэк веб-приложение для того чтобы использовать данные нейросети.

Так же возможно улучшить качестве генерации контента заменой наших моделей на более мощные модели, более глубоким анализом и обработкой контента ну и конечно-же дообучением на более большом корпусе данных

